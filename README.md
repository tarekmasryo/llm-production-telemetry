<h2 style="margin:0 0 10px 0;">LLM Production Telemetry â€” Decisionâ€‘Grade Observability</h2>
<div style="margin:0 0 14px 0; opacity:0.9;">
  An <b>operator decision notebook</b>: turn noisy LLM telemetry into <b>shipâ€‘ready policies</b> â€”
  SLO/budget burn â†’ hotspots â†’ routing backtest â†’ drift checks â†’ capacityâ€‘aware triage â†’ <code>DecisionArtifact</code>.
</div>

---

## What this repo is ğŸ§­
This repo contains a <b>productionâ€‘minded notebook</b> that converts multiâ€‘table LLM telemetry into <b>versionable policy artifacts</b> you can:

- review like configs âœ…
- deploy behind feature flags ğŸš¦
- monitor + rollback safely ğŸ”

This is not â€œEDA for pretty chartsâ€. It is a <b>decision pipeline</b>.

---

## What you will produce ğŸ“¦
The notebook writes outputs to <code>artifacts/</code>:

- <code>routing_policy_use_case.csv</code> â€” routing policy per <code>use_case</code> (costâ€‘aware + SLOâ€‘aware)
- <code>drift_report.csv</code> â€” drift signals across windows (PSI / totalâ€‘variation distance)
- <code>triage_threshold_policy.json</code> â€” capacityâ€‘aware review threshold (risk Ã— unit costs Ã— workload)
- <code>triage_actions_preview.csv</code> â€” ranked reviewâ€‘queue preview in the evaluation window
- <code>decision_artifact.json</code> â€” strict JSON summary designed for automation/auditability

---

## Data inputs ğŸ—ƒï¸
<b>Required</b> (CSV):
- <code>llm_system_interactions.csv</code>
- <code>llm_system_sessions_summary.csv</code>
- <code>llm_system_users_summary.csv</code>

<b>Optional</b> (CSV, ignored if missing):
- <code>llm_system_prompts_lookup.csv</code>
- <code>llm_system_instruction_tuning_samples.csv</code>

Schema notes: see <code>docs/schema.md</code>.

<b>Discovery order</b> (first hit wins):
<code>$LLMOPS_DATA_DIR</code> â†’ <code>./</code> â†’ <code>./data</code> â†’ <code>/mnt/data</code> â†’ <code>/kaggle/input</code>

---

## How to run âš™ï¸
### Option A â€” Kaggle
1) Add your dataset (or upload the CSVs).
2) Open <code>LLM_Production_Telemetry.ipynb</code> and click <b>Run All</b>.
3) Download outputs from <code>artifacts/</code>.

### Option B â€” Local (interactive)
Recommended: <b>Python 3.11+</b>

```bash
python -m venv .venv
# Windows (PowerShell): .venv\Scripts\Activate.ps1
# macOS/Linux: source .venv/bin/activate

python -m pip install -U pip
pip install -r requirements.txt
```

(Optional) dev tooling:
```bash
pip install -r requirements-dev.txt
```

Point the notebook to your CSVs:
- Windows (PowerShell)
  ```powershell
  $env:LLMOPS_DATA_DIR="D:\\path\\to\\csvs"
  ```
- macOS/Linux
  ```bash
  export LLMOPS_DATA_DIR=/path/to/csvs
  ```

Open the notebook:
```bash
jupyter notebook
```

### Option C â€” Headless execution (CIâ€‘friendly)
Generate sample telemetry (safe to share), validate, then execute:

```bash
python scripts/generate_sample_data.py --out-dir data/sample --n-users 300 --n-sessions 500 --n-interactions 2400 --seed 42
python scripts/validate_data.py --data-dir data/sample
python scripts/run_notebook.py --data-dir data/sample --out-dir artifacts --config configs/default.yaml
```

---

## Windowing & leakage protection ğŸ§ 
- If a <code>split</code> column exists (<code>train/val/test</code>), the notebook uses it.
- Otherwise, it uses a <b>sessionâ€‘safe time split</b> so sessions never leak across windows.

---

## Notebook flow (reader path) ğŸ”
1) Integrity gates (PK/FK + token sanity)
2) Health snapshot (failure/SLA/cost + missingness)
3) Budget burn over time
4) Hotspots + risk slices (where it breaks)
5) Routing policy + backtest (policy candidates + impact estimate)
6) Drift report (what changed between windows)
7) Triage threshold (calibrated risk â†’ capacityâ€‘aware decision)
8) DecisionArtifact (machineâ€‘readable summary)

---

## Configuration knobs ğŸ§©
Policy knobs live in <code>configs/default.yaml</code> and are mirrored to environment variables.
Environment variables take precedence over YAML.

Example overrides:
- Windows (PowerShell)
  ```powershell
  $env:SLA_MS="2000"
  $env:DAILY_COST_BUDGET_USD="50"
  python scripts/run_notebook.py --data-dir data/sample --out-dir artifacts
  ```
- macOS/Linux
  ```bash
  export SLA_MS=2000
  export DAILY_COST_BUDGET_USD=50
  python scripts/run_notebook.py --data-dir data/sample --out-dir artifacts
  ```

---

## Quality gates âœ…
Run inside the venv:

```bash
ruff check .
ruff format --check .

pytest --cov=scripts --cov-report=term-missing
```

Pre-commit (requires a Git repo; ZIP users can run <code>git init</code> first):
```bash
pre-commit install
pre-commit run --all-files
```

---

## Important disclaimers âš ï¸
- <b>Routing backtest has selection bias.</b> This is an observational estimate from historical behavior â€” treat it as a candidate policy to test behind a flag.
- <b>Missing cost/latency can distort decisions.</b> If your telemetry drops these fields at nonâ€‘trivial rates, add stopâ€‘ship gates or estimate via pricing + tokens.
- <b>Triage is postâ€‘call failure triage.</b> The label is <code>is_failure</code>, not a generic â€œneeds_human_reviewâ€ signal unless your schema defines it.

---

## Repo structure ğŸ§±
```text
llm-production-telemetry/
â”œâ”€ LLM_Production_Telemetry.ipynb
â”œâ”€ artifacts/                      # generated outputs (gitignored)
â”œâ”€ configs/
â”‚  â””â”€ default.yaml                 # policy knobs (SLA/budgets/capacity)
â”œâ”€ data/
â”‚  â””â”€ sample/                      # synthetic, safe-to-share telemetry
â”œâ”€ docs/
â”‚  â”œâ”€ schema.md
â”‚  â””â”€ outputs.md
â”œâ”€ scripts/
â”‚  â”œâ”€ generate_sample_data.py
â”‚  â”œâ”€ validate_data.py
â”‚  â””â”€ run_notebook.py
â””â”€ .github/workflows/ci.yml
```

---

## License
MIT â€” see <code>LICENSE</code>.
