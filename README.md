<h1 style="margin:0 0 8px 0;">LLM Production Telemetry â€” Decision-Grade Observability</h1>
<div style="margin:0 0 14px 0; opacity:0.9;">
An <b>operator decision notebook</b>: turn noisy LLM telemetry into <b>ship-ready policies</b> â€”
SLO/budget burn â†’ hotspots â†’ routing backtest â†’ drift checks â†’ capacity-aware triage.
</div>

---

## What this repo is ğŸ§­
This repo contains a **production-minded notebook** that converts multi-table LLM telemetry into **versionable policy artifacts** you can:

- review like configs âœ…  
- deploy behind feature flags ğŸš¦  
- monitor + rollback safely ğŸ”  

This is not â€œEDA for pretty chartsâ€. Itâ€™s a **decision pipeline**.

---

## What you will produce ğŸ“¦
The notebook writes outputs to `artifacts/`:

- `routing_policy_use_case.csv` â€” routing policy per `use_case` (cost-aware + SLO-aware) ğŸ§©  
- `drift_report.csv` â€” drift signals across windows (PSI / TV distance) to flag policy decay ğŸ“‰  
- `triage_threshold_policy.json` â€” capacity-aware review threshold (risk Ã— unit costs Ã— workload) ğŸ›¡ï¸  
- `triage_actions_preview.csv` â€” ranked review-queue preview in the evaluation window ğŸ—‚ï¸  
- `decision_artifact.json` â€” strict JSON summary designed for automation/auditability ğŸ§¾  

---

## Data inputs ğŸ—ƒï¸
**Required**
- `llm_system_interactions.csv`
- `llm_system_sessions_summary.csv`
- `llm_system_users_summary.csv`

**Optional**
- `llm_system_prompts_lookup.csv`
- `llm_system_instruction_tuning_samples.csv`

The notebook searches these locations (in order):  
`$LLMOPS_DATA_DIR`, `./`, `./data`, `/mnt/data`, and Kaggle input folders.

---

## How to run âš™ï¸
### Option A â€” Kaggle
1) Add the dataset (or upload the CSVs).  
2) Open the notebook and click **Run All**.  
3) Download the generated files from `artifacts/`.

### Option B â€” Local
Create a virtual environment and install dependencies:

```bash
python -m venv .venv
source .venv/bin/activate  # (Windows) .venv\Scripts\activate

pip install -U pip
pip install numpy pandas matplotlib seaborn scikit-learn
```

Place the CSVs in `./data/` **or** set:

```bash
export LLMOPS_DATA_DIR="/path/to/your/csvs"
```

Then run the notebook end-to-end (Jupyter/VS Code).

---

## Windowing & leakage protection ğŸ§ 
- If a `split` column exists (with `train/val/test`), the notebook uses it.
- Otherwise, it uses a **session-safe time split** to avoid leakage: sessions are assigned fully to history vs evaluation.

---

## Notebook flow (reader path) ğŸ”
1) **Integrity gates** (PK/FK + token sanity)  
2) **Health snapshot** (failure/SLA/cost + missingness)  
3) **Budget burn** over time  
4) **Hotspots + risk slices** (where it breaks)  
5) **Routing policy + backtest** (policy candidates + impact estimate)  
6) **Drift report** (what changed between windows)  
7) **Triage threshold** (calibrated risk â†’ capacity-aware decision)  
8) **DecisionArtifact** (machine-readable summary)

---

## Compatibility notes ğŸ§©
- **scikit-learn >= 1.6**: calibration avoids deprecated `cv="prefit"` behavior (via the recommended pattern), so you shouldnâ€™t see the `cv='prefit' is deprecated` warning on recent versions.

---

## Important disclaimers âš ï¸
- **Routing backtest has selection bias.** This is an *observational* estimate based on historical behavior. It does not prove causal improvement; treat it as a candidate policy to test behind a flag (or via A/B).  
- **Missing `cost_usd` / `latency_ms` can distort decisions.** If your telemetry is missing these fields at non-trivial rates, add stop-ship gates or estimate cost/latency via pricing + tokens.  
- **Triage here is post-call failure triage.** The label is `is_failure` (a failure happened), not a generic â€œneeds_human_reviewâ€ signal unless your schema defines it.

---

## Repo structure ğŸ§±
```text
.
â”œâ”€â”€ LLM_Production_Telemetry.ipynb
â”œâ”€â”€ artifacts/                 # generated by the notebook
â”œâ”€â”€ data/                      # optional local CSV placement
â””â”€â”€ README.md
```
