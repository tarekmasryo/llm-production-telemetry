{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "989a531f",
   "metadata": {},
   "source": [
    "<h1 style=\"margin:0 0 8px 0;\">LLM Production Telemetry — Decision-Grade Observability</h1>\n",
    "<div style=\"margin:0 0 10px 0; opacity:0.9;\">\n",
    "Operator notebook: convert telemetry into <b>ship-ready policies</b> — SLO/budget burn → hotspots → routing backtest → drift checks → capacity-aware triage.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc76e61",
   "metadata": {},
   "source": [
    "<div style=\"padding:16px 18px; border:1px solid rgba(255,255,255,0.12); border-radius:16px; background:rgba(255,255,255,0.04);\">\n",
    "  <b>Story setup</b><br/>\n",
    "  You are the on-call operator for an LLM system. Telemetry is noisy, budgets are limited, and failures have real impact.\n",
    "  This notebook turns multi-table telemetry into <b>versionable policies</b> you can review, ship, and monitor.\n",
    "  <br/><br/>\n",
    "\n",
    "  <b>The decisions you’ll make</b><br/>\n",
    "  1) Are we burning reliability/SLA budgets — and where?<br/>\n",
    "  2) What routing + triage policy minimizes cost while staying inside constraints?\n",
    "  <br/><br/>\n",
    "\n",
    "  <b>What you will produce</b><br/>\n",
    "  • <code>routing_policy_use_case.csv</code> — routing policy per use case (cost-aware + SLO-aware)<br/>\n",
    "  • <code>drift_report.csv</code> — changes across time windows (PSI / TV distance) to flag policy decay<br/>\n",
    "  • <code>triage_threshold_policy.json</code> — a review threshold based on capacity, risk, and unit costs<br/>\n",
    "  • <code>triage_actions_preview.csv</code> — a ranked preview of the review queue in the evaluation window<br/>\n",
    "  • <code>decision_artifact.json</code> — a strict JSON summary intended for automation\n",
    "  <br/><br/>\n",
    "\n",
    "  <b>How to run</b><br/>\n",
    "  1) Set the <code>Operator knobs</code> (SLO, budgets, unit costs, review capacity).<br/>\n",
    "  2) Run cells top→bottom. If an integrity gate fails, stop and fix inputs before using downstream results.<br/>\n",
    "  3) Treat outputs as <i>policies</i>: review, version, and monitor them over time.\n",
    "  <br/><br/>\n",
    "\n",
    "  <b>Key assumptions</b><br/>\n",
    "  • Failure is defined by <code>is_failure == 1</code> (or derived from <code>failure_type != \"none\"</code>).<br/>\n",
    "  • “Review” is an operator queue action with a per-review cost; false negatives represent business risk.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3abb57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from IPython import get_ipython\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "\n",
    "ip = get_ipython()\n",
    "if ip is not None:\n",
    "    try:\n",
    "        ip.run_line_magic(\"matplotlib\", \"inline\")\n",
    "        ip.run_line_magic(\"config\", \"InlineBackend.figure_format='retina'\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 300)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "pd.set_option(\"compute.use_numexpr\", False)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "SLA_MS = 2200\n",
    "\n",
    "# SLO targets (\"budgets\"): what healthy steady-state should look like.\n",
    "FAIL_BUDGET = 0.02\n",
    "SLA_BUDGET = 0.05\n",
    "\n",
    "# One consistent historical vs evaluation split used across routing / drift / triage.\n",
    "TIME_SPLIT_FRAC = 0.80  # 80% history, 20% evaluation\n",
    "\n",
    "# Routing guardrails (not the same as budgets).\n",
    "# Budgets describe steady-state targets; routing guardrails are temporary mitigation constraints.\n",
    "ROUTING_FAIL_BUDGET_MULT = 6.0\n",
    "ROUTING_MAX_FAIL_RATE = float(min(0.20, max(FAIL_BUDGET * ROUTING_FAIL_BUDGET_MULT, FAIL_BUDGET)))\n",
    "\n",
    "# For visualization / hotspot triage, use a softer \"alert\" threshold than the strict budget target.\n",
    "HOTSPOT_FAIL_RATE = float(max(FAIL_BUDGET * 3.0, 0.05))\n",
    "\n",
    "MIN_REQUESTS_POLICY = 60\n",
    "MIN_REQUESTS_SLICE = 60\n",
    "\n",
    "# Data quality stop-ship gates: policies that depend on cost/latency require coverage.\n",
    "MAX_LATENCY_MISSING_RATE = 0.05\n",
    "MAX_COST_MISSING_RATE = 0.05\n",
    "\n",
    "# Economics\n",
    "REVIEW_COST = 2.0\n",
    "TRIAGE_FN_COST_USD = 40.0          # missed review risk (triage)\n",
    "FAILURE_EVENT_COST_USD = 40.0      # business loss when a request fails (routing)\n",
    "# Routing objective (expected unit cost): include business loss for failures + SLA breach penalty.\n",
    "SLA_BREACH_PENALTY_USD = 5.0\n",
    "\n",
    "CAPACITY_PER_DAY = 200\n",
    "CAPACITY_P90_GUARD = 250\n",
    "\n",
    "DAILY_COST_BUDGET_USD = 60.0\n",
    "\n",
    "OUT_DIR = Path(\"artifacts\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams.update({\"figure.figsize\": (11, 4.5), \"figure.dpi\": 120})\n",
    "\n",
    "\n",
    "TRUE_SET = {\"true\", \"1\", \"yes\", \"y\", \"t\"}\n",
    "FALSE_SET = {\"false\", \"0\", \"no\", \"n\", \"f\", \"\"}\n",
    "\n",
    "\n",
    "def coerce_bool_series(s: pd.Series, default: bool = False) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Robust boolean coercion for telemetry flags.\n",
    "\n",
    "    Accepts: bools, 0/1, strings like \"true\"/\"false\", \"yes\"/\"no\".\n",
    "    Unknown values fall back to `default`.\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return pd.Series(default, index=None, dtype=\"bool\")\n",
    "\n",
    "    if pd.api.types.is_bool_dtype(s):\n",
    "        return s.fillna(default).astype(bool)\n",
    "\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        v = pd.to_numeric(s, errors=\"coerce\").fillna(0)\n",
    "        return v.ne(0)\n",
    "\n",
    "    ss = s.astype(\"string\").str.strip().str.lower()\n",
    "    is_true = ss.isin(TRUE_SET)\n",
    "    is_false = ss.isin(FALSE_SET) | ss.isna()\n",
    "    out = np.where(is_true, True, np.where(is_false, False, default))\n",
    "    return pd.Series(out, index=s.index, dtype=\"bool\")\n",
    "\n",
    "\n",
    "def q(x: pd.Series, p: float) -> float:\n",
    "    v = pd.to_numeric(x, errors=\"coerce\").dropna().astype(float).values\n",
    "    return float(np.percentile(v, p)) if len(v) else float(\"nan\")\n",
    "\n",
    "\n",
    "def ensure_col(df: pd.DataFrame, col: str, default):\n",
    "    if col not in df.columns:\n",
    "        df[col] = default\n",
    "\n",
    "\n",
    "def require_cols(df: pd.DataFrame, cols: list[str], name: str):\n",
    "    miss = [c for c in cols if c not in df.columns]\n",
    "    if miss:\n",
    "        raise KeyError(f\"{name} missing columns: {miss}\")\n",
    "\n",
    "\n",
    "def safe_mean(s) -> float:\n",
    "    return float(pd.to_numeric(s, errors=\"coerce\").mean())\n",
    "\n",
    "\n",
    "def safe_sum(s) -> float:\n",
    "    return float(pd.to_numeric(s, errors=\"coerce\").sum())\n",
    "\n",
    "\n",
    "def breach_rate_non_null(latency_ms: pd.Series, sla_ms: float) -> float:\n",
    "    lat = pd.to_numeric(latency_ms, errors=\"coerce\")\n",
    "    nn = lat.notna()\n",
    "    if int(nn.sum()) == 0:\n",
    "        return float(\"nan\")\n",
    "    return float((lat[nn] > sla_ms).mean())\n",
    "\n",
    "\n",
    "def show_table(df: pd.DataFrame, title: str | None = None, max_rows: int = 30):\n",
    "    if title:\n",
    "        print(title)\n",
    "    display(df.head(max_rows))\n",
    "\n",
    "\n",
    "def line_chart(df: pd.DataFrame, x: str, ys: list[str], title: str, hlines: list[tuple[float, str]] | None = None):\n",
    "    ax = df.plot(x=x, y=ys, kind=\"line\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if hlines:\n",
    "        for y, ls in hlines:\n",
    "            ax.axhline(y=y, linestyle=ls)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _read_csv_safe(p: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(p) if p.exists() else pd.DataFrame()\n",
    "\n",
    "\n",
    "def _read_json_safe(p: Path) -> dict:\n",
    "    if not p.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99828311",
   "metadata": {},
   "source": [
    "<h2 style=\"margin:24px 0 10px 0;\">Load data</h2>\n",
    "<div style=\"padding:14px 16px; border:1px solid rgba(255,255,255,0.12); border-radius:14px; background:rgba(255,255,255,0.04);\">\n",
    "<b>Why it matters</b><br/>\n",
    "If the notebook can’t reliably find your tables, everything downstream becomes fragile.\n",
    "This loader searches common locations (local, <code>/mnt/data</code>, Kaggle input) and fails fast if required files are missing.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30881293",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQ = [\n",
    "    \"llm_system_interactions.csv\",\n",
    "    \"llm_system_sessions_summary.csv\",\n",
    "    \"llm_system_users_summary.csv\",\n",
    "]\n",
    "OPT = [\n",
    "    \"llm_system_prompts_lookup.csv\",\n",
    "    \"llm_system_instruction_tuning_samples.csv\",\n",
    "]\n",
    "\n",
    "\n",
    "def _find_file(root: Path, filename: str) -> Path | None:\n",
    "    root = root.expanduser().resolve()\n",
    "    p = root / filename\n",
    "    if p.exists():\n",
    "        return p\n",
    "\n",
    "    if str(root) == \"/kaggle/input\" and root.exists():\n",
    "        try:\n",
    "            for d in root.iterdir():\n",
    "                if d.is_dir():\n",
    "                    p2 = d / filename\n",
    "                    if p2.exists():\n",
    "                        return p2\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        for hit in root.rglob(filename):\n",
    "            if hit.exists():\n",
    "                return hit\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def resolve_paths() -> dict[str, Path]:\n",
    "    candidates: list[Path] = []\n",
    "    env = os.getenv(\"LLMOPS_DATA_DIR\", \"\").strip()\n",
    "    if env:\n",
    "        candidates.append(Path(env))\n",
    "    candidates += [Path.cwd(), Path.cwd() / \"data\", Path(\"/mnt/data\")]\n",
    "    kroot = Path(\"/kaggle/input\")\n",
    "    if kroot.exists():\n",
    "        candidates.append(kroot)\n",
    "\n",
    "    found: dict[str, Path] = {}\n",
    "    for fn in REQ + OPT:\n",
    "        for root in candidates:\n",
    "            p = _find_file(root, fn)\n",
    "            if p is not None:\n",
    "                found[fn] = p\n",
    "                break\n",
    "    return found\n",
    "\n",
    "\n",
    "paths = resolve_paths()\n",
    "\n",
    "\n",
    "def read_csv(name: str) -> pd.DataFrame | None:\n",
    "    p = paths.get(name)\n",
    "    if p is None:\n",
    "        return None\n",
    "    return pd.read_csv(p)\n",
    "\n",
    "\n",
    "interactions = read_csv(\"llm_system_interactions.csv\")\n",
    "sessions = read_csv(\"llm_system_sessions_summary.csv\")\n",
    "users = read_csv(\"llm_system_users_summary.csv\")\n",
    "prompts = read_csv(\"llm_system_prompts_lookup.csv\")\n",
    "sft = read_csv(\"llm_system_instruction_tuning_samples.csv\")\n",
    "\n",
    "missing = [k for k, v in {\"llm_system_interactions.csv\": interactions, \"llm_system_sessions_summary.csv\": sessions, \"llm_system_users_summary.csv\": users}.items() if v is None]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Missing required CSV(s): {missing}. Resolved search paths: {sorted(set(str(p.parent) for p in paths.values()))}\")\n",
    "\n",
    "print(\"Loaded:\", interactions.shape, sessions.shape, users.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2432feb1",
   "metadata": {},
   "source": [
    "<h2 style=\"margin:24px 0 10px 0;\">Normalize types + integrity gates</h2>\n",
    "<div style=\"padding:14px 16px; border:1px solid rgba(255,255,255,0.12); border-radius:14px; background:rgba(255,255,255,0.04);\">\n",
    "<b>Hard gate</b><br/>\n",
    "We stop if primary keys are not unique, foreign keys don’t match, or token accounting is inconsistent.\n",
    "If this passes, later plots and policies are worth believing.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions.copy()\n",
    "sessions = sessions.copy()\n",
    "users = users.copy()\n",
    "\n",
    "interactions[\"timestamp_utc\"] = pd.to_datetime(interactions.get(\"timestamp_utc\"), utc=True, errors=\"coerce\")\n",
    "if interactions[\"timestamp_utc\"].notna().sum() == 0 and \"date_utc\" in interactions.columns:\n",
    "    d = pd.to_datetime(interactions[\"date_utc\"], utc=True, errors=\"coerce\")\n",
    "    h = pd.to_numeric(interactions.get(\"hour_of_day_utc\"), errors=\"coerce\").fillna(0).astype(int)\n",
    "    interactions[\"timestamp_utc\"] = d + pd.to_timedelta(h, unit=\"h\")\n",
    "\n",
    "for c in [\"start_timestamp_utc\", \"end_timestamp_utc\"]:\n",
    "    if c in sessions.columns:\n",
    "        sessions[c] = pd.to_datetime(sessions[c], utc=True, errors=\"coerce\")\n",
    "\n",
    "num_cols = [\n",
    "    \"latency_ms\",\n",
    "    \"prompt_tokens\",\n",
    "    \"completion_tokens\",\n",
    "    \"total_tokens\",\n",
    "    \"cost_usd\",\n",
    "    \"tool_calls_count\",\n",
    "    \"response_quality_score\",\n",
    "    \"user_feedback_score\",\n",
    "    \"tokens_per_second\",\n",
    "    \"temperature\",\n",
    "    \"top_p\",\n",
    "    \"max_tokens\",\n",
    "    \"retry_index\",\n",
    "]\n",
    "for c in num_cols:\n",
    "    if c in interactions.columns:\n",
    "        interactions[c] = pd.to_numeric(interactions[c], errors=\"coerce\")\n",
    "\n",
    "interactions.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "if \"tokens_per_second\" not in interactions.columns and {\"total_tokens\", \"latency_ms\"}.issubset(interactions.columns):\n",
    "    denom = pd.to_numeric(interactions[\"latency_ms\"], errors=\"coerce\") / 1000.0\n",
    "    interactions[\"tokens_per_second\"] = pd.to_numeric(interactions[\"total_tokens\"], errors=\"coerce\") / denom.replace(0, np.nan)\n",
    "\n",
    "if \"prompt_to_completion_ratio\" not in interactions.columns and {\"prompt_tokens\", \"completion_tokens\"}.issubset(interactions.columns):\n",
    "    pt = pd.to_numeric(interactions[\"prompt_tokens\"], errors=\"coerce\").round().astype(\"Int64\")\n",
    "    ct = pd.to_numeric(interactions[\"completion_tokens\"], errors=\"coerce\").round().astype(\"Int64\")\n",
    "    interactions[\"prompt_to_completion_ratio\"] = pt / ct.replace(0, np.nan)\n",
    "\n",
    "require_cols(interactions, [\"interaction_id\", \"session_id\", \"user_id\"], \"interactions\")\n",
    "require_cols(sessions, [\"session_id\", \"user_id\"], \"sessions\")\n",
    "require_cols(users, [\"user_id\"], \"users\")\n",
    "\n",
    "if not interactions[\"interaction_id\"].is_unique:\n",
    "    raise ValueError(\"interactions.interaction_id must be unique\")\n",
    "if not sessions[\"session_id\"].is_unique:\n",
    "    raise ValueError(\"sessions.session_id must be unique\")\n",
    "if not users[\"user_id\"].is_unique:\n",
    "    raise ValueError(\"users.user_id must be unique\")\n",
    "\n",
    "if not interactions[\"session_id\"].isin(sessions[\"session_id\"]).all():\n",
    "    raise ValueError(\"FK failed: interactions.session_id → sessions.session_id\")\n",
    "if not interactions[\"user_id\"].isin(users[\"user_id\"]).all():\n",
    "    raise ValueError(\"FK failed: interactions.user_id → users.user_id\")\n",
    "\n",
    "if sft is not None and \"interaction_id\" in sft.columns:\n",
    "    if not sft[\"interaction_id\"].is_unique:\n",
    "        raise ValueError(\"sft.interaction_id must be unique (1:1)\")\n",
    "    if not sft[\"interaction_id\"].isin(interactions[\"interaction_id\"]).all():\n",
    "        raise ValueError(\"FK failed: sft.interaction_id → interactions.interaction_id\")\n",
    "\n",
    "if {\"total_tokens\", \"prompt_tokens\", \"completion_tokens\"}.issubset(interactions.columns):\n",
    "    tt = pd.to_numeric(interactions[\"total_tokens\"], errors=\"coerce\").round().astype(\"Int64\")\n",
    "    pt = pd.to_numeric(interactions[\"prompt_tokens\"], errors=\"coerce\").round().astype(\"Int64\")\n",
    "    ct = pd.to_numeric(interactions[\"completion_tokens\"], errors=\"coerce\").round().astype(\"Int64\")\n",
    "    mask = tt.notna() & pt.notna() & ct.notna()\n",
    "    if mask.sum() == 0:\n",
    "        raise ValueError(\"Invariant check skipped: token columns exist but have no non-null rows\")\n",
    "    bad = tt[mask] != (pt[mask] + ct[mask])\n",
    "    if bad.any():\n",
    "        raise ValueError(\"Invariant violated: total_tokens != prompt_tokens + completion_tokens (non-null rows)\")\n",
    "\n",
    "FLAG_COLS = [\n",
    "    \"safety_block_flag\",\n",
    "    \"hallucination_flag\",\n",
    "    \"toxicity_flag\",\n",
    "    \"formatting_error_flag\",\n",
    "    \"tool_error_flag\",\n",
    "    \"latency_timeout_flag\",\n",
    "]\n",
    "for c in FLAG_COLS:\n",
    "    ensure_col(interactions, c, False)\n",
    "    interactions[c] = coerce_bool_series(interactions[c], default=False)\n",
    "\n",
    "ensure_col(interactions, \"is_failure\", np.nan)\n",
    "interactions[\"is_failure\"] = pd.to_numeric(interactions[\"is_failure\"], errors=\"coerce\")\n",
    "interactions[\"is_failure_was_missing\"] = interactions[\"is_failure\"].isna()\n",
    "\n",
    "if \"failure_type\" in interactions.columns:\n",
    "    ft = interactions[\"failure_type\"].astype(\"string\").fillna(\"none\").str.strip().str.lower()\n",
    "    derived = (ft != \"none\").astype(\"int64\")\n",
    "else:\n",
    "    derived = pd.Series(0, index=interactions.index, dtype=\"int64\")\n",
    "\n",
    "na_mask = interactions[\"is_failure\"].isna()\n",
    "if na_mask.any():\n",
    "    interactions.loc[na_mask, \"is_failure\"] = derived.loc[na_mask]\n",
    "\n",
    "interactions[\"is_failure\"] = (\n",
    "    pd.to_numeric(interactions[\"is_failure\"], errors=\"coerce\")\n",
    "    .fillna(0)\n",
    "    .astype(\"int64\")\n",
    "    .clip(0, 1)\n",
    ")\n",
    "\n",
    "print(\"Integrity OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d55c9ef",
   "metadata": {},
   "source": [
    "<h2 style='margin:22px 0 10px 0;'>Join sessions + users</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7011c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = interactions.copy()\n",
    "s = sessions.copy()\n",
    "u = users.copy()\n",
    "\n",
    "for c in [\"use_case\", \"account_tier\", \"channel\", \"model_provider\", \"model_name\", \"region\", \"segment\"]:\n",
    "    ensure_col(i, c, \"unknown\")\n",
    "\n",
    "for c in [\"session_total_cost_usd\", \"session_total_tokens\", \"session_total_requests\", \"session_total_failures\"]:\n",
    "    ensure_col(s, c, np.nan)\n",
    "\n",
    "for c in [\"user_total_cost_usd\", \"user_total_requests\", \"user_total_failures\"]:\n",
    "    ensure_col(u, c, np.nan)\n",
    "\n",
    "i[\"session_id\"] = i[\"session_id\"].astype(str)\n",
    "i[\"user_id\"] = i[\"user_id\"].astype(str)\n",
    "s[\"session_id\"] = s[\"session_id\"].astype(str)\n",
    "s[\"user_id\"] = s[\"user_id\"].astype(str)\n",
    "u[\"user_id\"] = u[\"user_id\"].astype(str)\n",
    "\n",
    "inter = i.merge(\n",
    "    s.drop(columns=[c for c in [\"user_id\"] if c in s.columns], errors=\"ignore\"),\n",
    "    on=\"session_id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_session\"),\n",
    ")\n",
    "\n",
    "inter = inter.merge(\n",
    "    u,\n",
    "    on=\"user_id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_user\"),\n",
    ")\n",
    "\n",
    "df_time = inter.dropna(subset=[\"timestamp_utc\"]).sort_values(\"timestamp_utc\").reset_index(drop=True)\n",
    "\n",
    "if len(df_time) == 0:\n",
    "    train_hist = df_time.copy()\n",
    "    test_future = df_time.copy()\n",
    "    cut_ts = None\n",
    "else:\n",
    "    cut_idx = int(TIME_SPLIT_FRAC * len(df_time))\n",
    "    cut_ts = df_time.loc[cut_idx, \"timestamp_utc\"]\n",
    "    train_hist = df_time[df_time[\"timestamp_utc\"] <= cut_ts].copy()\n",
    "    test_future = df_time[df_time[\"timestamp_utc\"] > cut_ts].copy()\n",
    "\n",
    "print(\"Time split:\", cut_ts, train_hist.shape, test_future.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297febf4",
   "metadata": {},
   "source": [
    "<h2 style=\"margin:24px 0 10px 0;\">1) Health snapshot</h2>\n",
    "<div style=\"padding:14px 16px; border:1px solid rgba(255,255,255,0.12); border-radius:14px; background:rgba(255,255,255,0.04);\">\n",
    "<b>First question</b><br/>\n",
    "Are we healthy right now? We summarize failure rate, SLA breach rate, p95 latency, cost, and tool/safety usage.\n",
    "This tells you if you’re in “monitor” mode or “incident” mode.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d32eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_col(inter, \"failure_type\", \"unknown\")\n",
    "ensure_col(inter, \"latency_ms\", np.nan)\n",
    "ensure_col(inter, \"cost_usd\", np.nan)\n",
    "ensure_col(inter, \"tool_calls_count\", 0)\n",
    "\n",
    "flags = [\n",
    "    \"safety_block_flag\",\n",
    "    \"hallucination_flag\",\n",
    "    \"toxicity_flag\",\n",
    "    \"formatting_error_flag\",\n",
    "    \"tool_error_flag\",\n",
    "    \"latency_timeout_flag\",\n",
    "]\n",
    "for c in flags:\n",
    "    ensure_col(inter, c, False)\n",
    "    inter[c] = coerce_bool_series(inter[c], default=False)\n",
    "\n",
    "lat = pd.to_numeric(inter[\"latency_ms\"], errors=\"coerce\")\n",
    "tc = pd.to_numeric(inter[\"tool_calls_count\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "lat_non_null = lat.notna()\n",
    "lat_missing_rate = float((~lat_non_null).mean())\n",
    "\n",
    "cost_num = pd.to_numeric(inter[\"cost_usd\"], errors=\"coerce\")\n",
    "cost_missing_rate = float(cost_num.isna().mean())\n",
    "\n",
    "is_fail_missing = pd.Series(False, index=inter.index)\n",
    "if \"is_failure_was_missing\" in inter.columns:\n",
    "    is_fail_missing = coerce_bool_series(inter[\"is_failure_was_missing\"], default=False)\n",
    "failure_missing_rate = float(is_fail_missing.mean())\n",
    "\n",
    "sla_breach_rate_nn = float((lat[lat_non_null] > SLA_MS).mean()) if int(lat_non_null.sum()) else float(\"nan\")\n",
    "\n",
    "if np.isfinite(lat_missing_rate) and (lat_missing_rate > MAX_LATENCY_MISSING_RATE):\n",
    "    raise ValueError(\n",
    "        f\"Stop-ship: latency_ms missing rate {lat_missing_rate:.2%} exceeds \"\n",
    "        f\"MAX_LATENCY_MISSING_RATE={MAX_LATENCY_MISSING_RATE:.2%}. \"\n",
    "        \"Fill/estimate latency before trusting SLO burn, routing, or triage outputs.\"\n",
    "    )\n",
    "if np.isfinite(cost_missing_rate) and (cost_missing_rate > MAX_COST_MISSING_RATE):\n",
    "    raise ValueError(\n",
    "        f\"Stop-ship: cost_usd missing rate {cost_missing_rate:.2%} exceeds \"\n",
    "        f\"MAX_COST_MISSING_RATE={MAX_COST_MISSING_RATE:.2%}. \"\n",
    "        \"Fill/estimate costs (or provide a pricing table) before trusting cost-aware policies.\"\n",
    "    )\n",
    "\n",
    "\n",
    "kpis = pd.DataFrame(\n",
    "    [\n",
    "        (\"Interactions\", f\"{len(inter):,}\"),\n",
    "        (\"Failure rate\", f\"{safe_mean(inter['is_failure']) * 100:.2f}%\"),\n",
    "        (\"SLA breach rate (non-null latency)\", f\"{sla_breach_rate_nn * 100:.2f}%\"),\n",
    "        (\"latency_ms missing rate\", f\"{lat_missing_rate * 100:.2f}%\"),\n",
    "        (\"cost_usd missing rate\", f\"{cost_missing_rate * 100:.2f}%\"),\n",
    "        (\"is_failure missing rate (raw)\", f\"{failure_missing_rate * 100:.2f}%\"),\n",
    "        (\"p95 latency (ms)\", f\"{q(lat, 95):.0f}\"),\n",
    "        (\"Total cost (estimated)\", f\"${safe_sum(inter['cost_usd']):,.2f}\"),\n",
    "        (\"Avg cost/request\", f\"${safe_mean(inter['cost_usd']):.4f}\"),\n",
    "        (\"Tool usage share\", f\"{float((tc > 0).mean()) * 100:.2f}%\"),\n",
    "        (\"Safety blocks share\", f\"{float(inter['safety_block_flag'].astype(bool).mean()) * 100:.2f}%\"),\n",
    "    ],\n",
    "    columns=[\"Metric\", \"Value\"],\n",
    ")\n",
    "show_table(kpis, title=\"System snapshot\", max_rows=50)\n",
    "\n",
    "mix = (\n",
    "    inter[\"failure_type\"]\n",
    "    .astype(\"string\")\n",
    "    .fillna(\"unknown\")\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .value_counts(normalize=True)\n",
    "    .head(12)\n",
    "    .reset_index()\n",
    ")\n",
    "mix.columns = [\"failure_type\", \"share\"]\n",
    "plt.figure()\n",
    "ax = sns.barplot(data=mix, x=\"failure_type\", y=\"share\")\n",
    "ax.set_title(\"Top failure types (share)\")\n",
    "ax.set_xlabel(\"failure_type\")\n",
    "ax.set_ylabel(\"share\")\n",
    "plt.xticks(rotation=25, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792cfb79",
   "metadata": {},
   "source": [
    "<h2 style='margin:22px 0 10px 0;'>2) SLO / error budget burn</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f079f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_time) == 0:\n",
    "    print(\"Skipped: no time data.\")\n",
    "else:\n",
    "    t = df_time.copy()\n",
    "    t[\"day\"] = t[\"timestamp_utc\"].dt.floor(\"D\")\n",
    "\n",
    "    lat_num = pd.to_numeric(t[\"latency_ms\"], errors=\"coerce\")\n",
    "    t[\"sla_breach\"] = np.where(lat_num.notna(), lat_num > SLA_MS, np.nan)\n",
    "\n",
    "    daily = (\n",
    "        t.groupby(\"day\", as_index=False)\n",
    "        .agg(\n",
    "            requests=(\"interaction_id\", \"size\"),\n",
    "            fail_rate=(\"is_failure\", \"mean\"),\n",
    "            sla_breach_rate=(\"sla_breach\", \"mean\"),\n",
    "            cost=(\"cost_usd\", \"sum\"),\n",
    "            p95_latency=(\"latency_ms\", lambda x: q(x, 95)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    daily[\"fail_budget_burn\"] = daily[\"fail_rate\"] / FAIL_BUDGET\n",
    "    daily[\"sla_budget_burn\"] = daily[\"sla_breach_rate\"] / SLA_BUDGET\n",
    "    daily[\"cost_budget_burn\"] = daily[\"cost\"] / max(DAILY_COST_BUDGET_USD, 1e-9)\n",
    "\n",
    "    line_chart(daily, x=\"day\", ys=[\"requests\"], title=\"Workload\")\n",
    "    line_chart(daily, x=\"day\", ys=[\"fail_rate\", \"sla_breach_rate\"], title=\"Failure + SLA breach rates\")\n",
    "    line_chart(daily, x=\"day\", ys=[\"fail_budget_burn\", \"sla_budget_burn\"], title=\"Error budget burn (rate / budget)\", hlines=[(1.0, \"--\")])\n",
    "    line_chart(daily, x=\"day\", ys=[\"cost_budget_burn\"], title=\"Daily cost burn (cost / budget)\", hlines=[(1.0, \"--\")])\n",
    "\n",
    "    show_table(daily.sort_values(\"day\", ascending=False).head(10), title=\"Daily summary (latest)\", max_rows=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e371a2a5",
   "metadata": {},
   "source": [
    "<h2 style='margin:22px 0 10px 0;'>3) Risk slices</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf31097",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train_hist) == 0:\n",
    "    print(\"Skipped: no training history.\")\n",
    "else:\n",
    "    base = train_hist.copy()\n",
    "    for c in [\"use_case\", \"account_tier\"]:\n",
    "        ensure_col(base, c, \"unknown\")\n",
    "\n",
    "    s2 = (\n",
    "        base.groupby([\"use_case\", \"account_tier\"], as_index=False)\n",
    "        .agg(\n",
    "            requests=(\"interaction_id\", \"size\"),\n",
    "            fail_rate=(\"is_failure\", \"mean\"),\n",
    "            p95_latency=(\"latency_ms\", lambda x: q(x, 95)),\n",
    "            avg_cost=(\"cost_usd\", \"mean\"),\n",
    "        )\n",
    "    )\n",
    "    s2 = s2[s2[\"requests\"] >= MIN_REQUESTS_SLICE].copy()\n",
    "\n",
    "    if len(s2) == 0:\n",
    "        print(\"Not enough volume for slices.\")\n",
    "    else:\n",
    "        s2[\"heuristic_risk_score\"] = 2.0 * s2[\"fail_rate\"] + 0.5 * (s2[\"p95_latency\"] / max(SLA_MS, 1)) + 0.1 * s2[\"avg_cost\"].fillna(0)\n",
    "        top = s2.sort_values(\"heuristic_risk_score\", ascending=False).head(15)\n",
    "        show_table(top, title=\"Top risk slices (historical)\", max_rows=30)\n",
    "\n",
    "        heat = s2.pivot(index=\"account_tier\", columns=\"use_case\", values=\"fail_rate\")\n",
    "        plt.figure(figsize=(11, 5))\n",
    "        ax = sns.heatmap(heat, cmap=\"viridis\", cbar=True)\n",
    "        ax.set_title(\"Failure rate heatmap (account_tier × use_case)\")\n",
    "        plt.xticks(rotation=25, ha=\"right\")\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd5e01",
   "metadata": {},
   "source": [
    "<h2 style=\"margin:24px 0 10px 0;\">Provider/model hotspots</h2>\n",
    "<div style=\"padding:14px 16px; border:1px solid rgba(255,255,255,0.12); border-radius:14px; background:rgba(255,255,255,0.04);\">\n",
    "<b>Routing intuition</b><br/>\n",
    "A simple risk map: latency vs failure. Points right of the SLA line are latency risk; points above the failure line are reliability risk.\n",
    "These hotspots become candidates for routing policy changes (or temporary bans).\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train_hist) == 0:\n",
    "    print(\"Skipped.\")\n",
    "else:\n",
    "    pm = (\n",
    "        train_hist.groupby([\"model_provider\", \"model_name\"], as_index=False)\n",
    "        .agg(\n",
    "            requests=(\"interaction_id\", \"size\"),\n",
    "            fail_rate=(\"is_failure\", \"mean\"),\n",
    "            p95_latency=(\"latency_ms\", lambda x: q(x, 95)),\n",
    "            cost_per_request=(\"cost_usd\", \"mean\"),\n",
    "            sla_breach_rate=(\"latency_ms\", lambda x: breach_rate_non_null(x, SLA_MS)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    pm = pm[pm[\"requests\"] >= MIN_REQUESTS_POLICY].copy()\n",
    "    pm[\"risk_bucket\"] = np.select(\n",
    "        [\n",
    "            (pm[\"fail_rate\"] > HOTSPOT_FAIL_RATE) & (pm[\"p95_latency\"] > SLA_MS),\n",
    "            (pm[\"fail_rate\"] > HOTSPOT_FAIL_RATE),\n",
    "            (pm[\"p95_latency\"] > SLA_MS),\n",
    "        ],\n",
    "        [\"high_failure_and_sla\", \"high_failure\", \"sla_risk\"],\n",
    "        default=\"ok\",\n",
    "    )\n",
    "\n",
    "    plot_df = pm.sort_values(\"requests\", ascending=False).head(30)\n",
    "    plt.figure(figsize=(11, 5))\n",
    "    ax = sns.scatterplot(\n",
    "        data=plot_df,\n",
    "        x=\"p95_latency\",\n",
    "        y=\"fail_rate\",\n",
    "        hue=\"risk_bucket\",\n",
    "        size=\"requests\",\n",
    "        sizes=(30, 420),\n",
    "        alpha=0.85,\n",
    "    )\n",
    "    ax.axvline(SLA_MS, linestyle=\"--\")\n",
    "    ax.axhline(HOTSPOT_FAIL_RATE, linestyle=\"--\")\n",
    "    ax.axhline(FAIL_BUDGET, linestyle=':', alpha=0.7)\n",
    "    ax.set_title(\"Provider/model risk map (historical)\")\n",
    "    ax.set_xlabel(\"p95 latency (ms)\")\n",
    "    ax.set_ylabel(\"Failure rate\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    show_table(pm.sort_values([\"risk_bucket\", \"requests\"], ascending=[True, False]).head(15), title=\"Provider/model hotspots (preview)\", max_rows=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1316538f",
   "metadata": {},
   "source": [
    "<h2 style='margin:22px 0 10px 0;'>4) Routing policy + backtest</h2>\n",
    "\n",
    "<div style=\"padding:14px 16px; border:1px solid rgba(255,255,255,0.12); border-radius:14px; background:rgba(255,255,255,0.04);\">\n",
    "  <b>Selection bias disclaimer (read this)</b><br/>\n",
    "  This is an <b>offline observational backtest</b>. It does not prove causality.<br/>\n",
    "  Provider/model performance is measured on the traffic that actually reached it, which can be biased by the current routing,\n",
    "  prompt mix, user segments, tool usage, or incident handling.<br/><br/>\n",
    "  Treat the routing output as a <b>candidate policy</b>. Validate with <b>shadow routing</b>, <b>A/B tests</b>, or controlled rollouts.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da2730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_path = OUT_DIR / \"routing_policy_use_case.csv\"\n",
    "\n",
    "def _pick_best(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    return (\n",
    "        df.sort_values(\n",
    "            [\"use_case\", \"expected_unit_cost\", \"p95_latency\", \"requests\"],\n",
    "            ascending=[True, True, True, False],\n",
    "        )\n",
    "        .groupby(\"use_case\", as_index=False)\n",
    "        .first()\n",
    "    )\n",
    "\n",
    "def build_policy_relaxed(\n",
    "    sla_ms: float,\n",
    "    max_fail: float,\n",
    "    failure_business_cost: float,\n",
    "    sla_breach_penalty_usd: float,\n",
    "    min_req: int = MIN_REQUESTS_POLICY,\n",
    ") -> pd.DataFrame:\n",
    "    grp_train = train_hist.groupby([\"use_case\", \"model_provider\", \"model_name\"], as_index=False).agg(\n",
    "        requests=(\"interaction_id\", \"size\"),\n",
    "        fail_rate=(\"is_failure\", \"mean\"),\n",
    "        avg_cost=(\"cost_usd\", \"mean\"),\n",
    "        p95_latency=(\"latency_ms\", lambda x: q(x, 95)),\n",
    "        sla_breach_rate=(\"latency_ms\", lambda x: breach_rate_non_null(x, sla_ms)),\n",
    "    )\n",
    "\n",
    "    base = grp_train[grp_train[\"requests\"] >= min_req].copy()\n",
    "\n",
    "    base[\"fail_rate\"] = pd.to_numeric(base[\"fail_rate\"], errors=\"coerce\")\n",
    "    base[\"avg_cost\"] = pd.to_numeric(base[\"avg_cost\"], errors=\"coerce\")\n",
    "    base[\"p95_latency\"] = pd.to_numeric(base[\"p95_latency\"], errors=\"coerce\")\n",
    "    base[\"sla_breach_rate\"] = pd.to_numeric(base[\"sla_breach_rate\"], errors=\"coerce\")\n",
    "    base = base.dropna(subset=[\"fail_rate\", \"avg_cost\", \"p95_latency\"])\n",
    "\n",
    "    if base.empty:\n",
    "        return base\n",
    "\n",
    "    base[\"expected_unit_cost\"] = (\n",
    "        base[\"avg_cost\"]\n",
    "        + base[\"fail_rate\"].clip(0, 1) * float(failure_business_cost)\n",
    "        + base[\"sla_breach_rate\"].fillna(0).clip(0, 1) * float(sla_breach_penalty_usd)\n",
    "    )\n",
    "\n",
    "    strict = base[(base[\"p95_latency\"] <= sla_ms) & (base[\"fail_rate\"] <= max_fail)].copy()\n",
    "    strict_best = _pick_best(strict)\n",
    "    strict_best[\"policy_mode\"] = \"strict\"\n",
    "\n",
    "    covered = set(strict_best[\"use_case\"].unique())\n",
    "    remaining = sorted(set(base[\"use_case\"].unique()) - covered)\n",
    "    picks = [strict_best] if len(strict_best) else []\n",
    "\n",
    "    relax_steps = [\n",
    "        (\"relax_1\", sla_ms * 1.15, max_fail + 0.05),\n",
    "        (\"relax_2\", sla_ms * 1.30, max_fail + 0.10),\n",
    "    ]\n",
    "    for mode, sla2, f2 in relax_steps:\n",
    "        if not remaining:\n",
    "            break\n",
    "        cand = base[base[\"use_case\"].isin(remaining)].copy()\n",
    "        cand = cand[(cand[\"p95_latency\"] <= sla2) & (cand[\"fail_rate\"] <= f2)].copy()\n",
    "        best = _pick_best(cand)\n",
    "        if len(best):\n",
    "            best[\"policy_mode\"] = mode\n",
    "            picks.append(best)\n",
    "            covered |= set(best[\"use_case\"].unique())\n",
    "            remaining = sorted(set(base[\"use_case\"].unique()) - covered)\n",
    "\n",
    "    if remaining:\n",
    "        fallback = _pick_best(base[base[\"use_case\"].isin(remaining)].copy())\n",
    "        if len(fallback):\n",
    "            fallback[\"policy_mode\"] = \"fallback_unconstrained\"\n",
    "            picks.append(fallback)\n",
    "\n",
    "    out = pd.concat(picks, ignore_index=True) if picks else pd.DataFrame()\n",
    "    cols = [\n",
    "        \"use_case\",\n",
    "        \"model_provider\",\n",
    "        \"model_name\",\n",
    "        \"requests\",\n",
    "        \"fail_rate\",\n",
    "        \"p95_latency\",\n",
    "        \"sla_breach_rate\",\n",
    "        \"avg_cost\",\n",
    "        \"expected_unit_cost\",\n",
    "        \"policy_mode\",\n",
    "    ]\n",
    "    return out[cols].sort_values(\"use_case\") if len(out) else out\n",
    "\n",
    "if len(train_hist) == 0 or len(test_future) == 0 or \"use_case\" not in train_hist.columns:\n",
    "    pd.DataFrame(\n",
    "        columns=[\n",
    "            \"use_case\",\n",
    "            \"model_provider\",\n",
    "            \"model_name\",\n",
    "            \"requests\",\n",
    "            \"fail_rate\",\n",
    "            \"p95_latency\",\n",
    "            \"sla_breach_rate\",\n",
    "            \"avg_cost\",\n",
    "            \"expected_unit_cost\",\n",
    "            \"policy_mode\",\n",
    "        ]\n",
    "    ).to_csv(policy_path, index=False)\n",
    "    print(\"Routing skipped: missing train/test window or use_case.\")\n",
    "else:\n",
    "    policy = build_policy_relaxed(\n",
    "        sla_ms=SLA_MS,\n",
    "        max_fail=ROUTING_MAX_FAIL_RATE,\n",
    "        failure_business_cost=FAILURE_EVENT_COST_USD,\n",
    "        sla_breach_penalty_usd=SLA_BREACH_PENALTY_USD,\n",
    "    )\n",
    "    policy.to_csv(policy_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    uc_base_test = test_future.groupby(\"use_case\", as_index=False).agg(\n",
    "        requests=(\"interaction_id\", \"size\"),\n",
    "        baseline_fail_rate=(\"is_failure\", \"mean\"),\n",
    "        baseline_avg_cost=(\"cost_usd\", \"mean\"),\n",
    "        baseline_p95_latency=(\"latency_ms\", lambda x: q(x, 95)),\n",
    "        baseline_sla_breach_rate=(\"latency_ms\", lambda x: breach_rate_non_null(x, SLA_MS)),\n",
    "    )\n",
    "\n",
    "    combo_test = test_future.groupby([\"use_case\", \"model_provider\", \"model_name\"], as_index=False).agg(\n",
    "        fail_rate=(\"is_failure\", \"mean\"),\n",
    "        avg_cost=(\"cost_usd\", \"mean\"),\n",
    "        p95_latency=(\"latency_ms\", lambda x: q(x, 95)),\n",
    "        sla_breach_rate=(\"latency_ms\", lambda x: breach_rate_non_null(x, SLA_MS)),\n",
    "    )\n",
    "\n",
    "    m = uc_base_test.merge(policy[[\"use_case\", \"model_provider\", \"model_name\", \"policy_mode\"]], on=\"use_case\", how=\"left\")\n",
    "    m = m.merge(combo_test, on=[\"use_case\", \"model_provider\", \"model_name\"], how=\"left\")\n",
    "\n",
    "    m[\"covered_by_policy\"] = m[\"model_name\"].notna()\n",
    "\n",
    "    for col, base_col in [\n",
    "        (\"fail_rate\", \"baseline_fail_rate\"),\n",
    "        (\"avg_cost\", \"baseline_avg_cost\"),\n",
    "        (\"p95_latency\", \"baseline_p95_latency\"),\n",
    "        (\"sla_breach_rate\", \"baseline_sla_breach_rate\"),\n",
    "    ]:\n",
    "        m[col] = pd.to_numeric(m[col], errors=\"coerce\").fillna(pd.to_numeric(m[base_col], errors=\"coerce\"))\n",
    "\n",
    "    req_total = float(m[\"requests\"].sum()) if \"requests\" in m.columns else float(\"nan\")\n",
    "\n",
    "    missing_any = m[[\"fail_rate\", \"avg_cost\", \"p95_latency\"]].isna().any(axis=1)\n",
    "    missing_share_requests = float((m.loc[missing_any, \"requests\"].sum() / req_total)) if req_total else 0.0\n",
    "    if missing_share_requests > 0:\n",
    "        raise ValueError(\n",
    "            f\"Stop-ship: routing backtest has missing metrics for {missing_share_requests:.2%} of requests \"\n",
    "            \"(after baseline fill). Provide cost/latency coverage or estimate them before using routing outputs.\"\n",
    "        )\n",
    "\n",
    "    m[\"baseline_expected_unit_cost\"] = (\n",
    "        pd.to_numeric(m[\"baseline_avg_cost\"], errors=\"coerce\")\n",
    "        + pd.to_numeric(m[\"baseline_fail_rate\"], errors=\"coerce\").clip(0, 1) * FAILURE_EVENT_COST_USD\n",
    "        + pd.to_numeric(m[\"baseline_sla_breach_rate\"], errors=\"coerce\").fillna(0).clip(0, 1) * SLA_BREACH_PENALTY_USD\n",
    "    )\n",
    "    m[\"policy_expected_unit_cost\"] = (\n",
    "        pd.to_numeric(m[\"avg_cost\"], errors=\"coerce\")\n",
    "        + pd.to_numeric(m[\"fail_rate\"], errors=\"coerce\").clip(0, 1) * FAILURE_EVENT_COST_USD\n",
    "        + pd.to_numeric(m[\"sla_breach_rate\"], errors=\"coerce\").fillna(0).clip(0, 1) * SLA_BREACH_PENALTY_USD\n",
    "    )\n",
    "\n",
    "    covered_uc = int(m.loc[m[\"covered_by_policy\"], \"use_case\"].nunique()) if \"use_case\" in m.columns else 0\n",
    "\n",
    "    impact = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"evaluation_window\": f\"timestamp_utc > {str(cut_ts)}\",\n",
    "                \"requests\": int(m[\"requests\"].sum()),\n",
    "                \"use_cases_total\": int(m[\"use_case\"].nunique()),\n",
    "                \"use_cases_covered\": covered_uc,\n",
    "                \"baseline_expected_fail_rate\": float((m[\"requests\"] * m[\"baseline_fail_rate\"]).sum() / m[\"requests\"].sum()),\n",
    "                \"policy_expected_fail_rate\": float((m[\"requests\"] * m[\"fail_rate\"]).sum() / m[\"requests\"].sum()),\n",
    "                \"baseline_expected_unit_cost\": float((m[\"requests\"] * m[\"baseline_expected_unit_cost\"]).sum() / m[\"requests\"].sum()),\n",
    "                \"policy_expected_unit_cost\": float((m[\"requests\"] * m[\"policy_expected_unit_cost\"]).sum() / m[\"requests\"].sum()),\n",
    "                \"baseline_total_expected_cost\": float((m[\"requests\"] * m[\"baseline_expected_unit_cost\"]).sum()),\n",
    "                \"policy_total_expected_cost\": float((m[\"requests\"] * m[\"policy_expected_unit_cost\"]).sum()),\n",
    "                \"routing_max_fail_rate\": float(ROUTING_MAX_FAIL_RATE),\n",
    "                \"failure_business_cost\": float(FAILURE_EVENT_COST_USD),\n",
    "                \"sla_breach_penalty_usd\": float(SLA_BREACH_PENALTY_USD),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    show_table(impact, title=\"Routing backtest summary (observational)\", max_rows=50)\n",
    "    print(\"Saved:\", policy_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a1878",
   "metadata": {},
   "source": [
    "<h2 style='margin:22px 0 10px 0;'>5) Drift report</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7740be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_path = OUT_DIR / \"drift_report.csv\"\n",
    "\n",
    "\n",
    "def psi(expected: np.ndarray, actual: np.ndarray, bins: int = 10) -> float:\n",
    "    expected = np.asarray(expected, dtype=float)\n",
    "    actual = np.asarray(actual, dtype=float)\n",
    "    expected = expected[np.isfinite(expected)]\n",
    "    actual = actual[np.isfinite(actual)]\n",
    "    if len(expected) < 50 or len(actual) < 50:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    edges = np.quantile(expected, np.linspace(0, 1, bins + 1))\n",
    "    edges = np.asarray(edges, dtype=float)\n",
    "    edges[0] = min(edges[0], float(np.min(actual)))\n",
    "    edges[-1] = max(edges[-1], float(np.max(actual)))\n",
    "    edges = np.unique(edges)\n",
    "    if len(edges) < 3:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    e_hist, _ = np.histogram(expected, bins=edges)\n",
    "    a_hist, _ = np.histogram(actual, bins=edges)\n",
    "\n",
    "    e = e_hist / max(e_hist.sum(), 1)\n",
    "    a = a_hist / max(a_hist.sum(), 1)\n",
    "\n",
    "    eps = 1e-6\n",
    "    e = np.clip(e, eps, 1)\n",
    "    a = np.clip(a, eps, 1)\n",
    "    return float(np.sum((a - e) * np.log(a / e)))\n",
    "\n",
    "\n",
    "def tv_distance(p: pd.Series, q_: pd.Series) -> float:\n",
    "    idx = p.index.union(q_.index)\n",
    "    p2 = p.reindex(idx, fill_value=0).astype(float)\n",
    "    q2 = q_.reindex(idx, fill_value=0).astype(float)\n",
    "    return float(0.5 * np.abs(p2 - q2).sum())\n",
    "\n",
    "\n",
    "if len(train_hist) == 0 or len(test_future) == 0:\n",
    "    pd.DataFrame().to_csv(drift_path, index=False)\n",
    "    print(\"Drift skipped.\")\n",
    "else:\n",
    "    num_cols = [c for c in [\"latency_ms\", \"cost_usd\", \"total_tokens\", \"prompt_tokens\", \"completion_tokens\", \"tool_calls_count\", \"response_quality_score\", \"tokens_per_second\", \"prompt_to_completion_ratio\"] if c in df_time.columns]\n",
    "    cat_cols = [c for c in [\"model_provider\", \"model_name\", \"use_case\", \"channel\", \"account_tier\", \"region\", \"segment\"] if c in df_time.columns]\n",
    "\n",
    "    rows = []\n",
    "    for c in num_cols:\n",
    "        rows.append({\"feature\": c, \"type\": \"numeric\", \"psi\": psi(train_hist[c].values, test_future[c].values, bins=10), \"tv_distance\": np.nan})\n",
    "    for c in cat_cols:\n",
    "        p = train_hist[c].astype(str).value_counts(normalize=True)\n",
    "        qv = test_future[c].astype(str).value_counts(normalize=True)\n",
    "        rows.append({\"feature\": c, \"type\": \"categorical\", \"psi\": np.nan, \"tv_distance\": tv_distance(p, qv)})\n",
    "\n",
    "    drift = pd.DataFrame(rows)\n",
    "\n",
    "    drift[\"psi\"] = pd.to_numeric(drift[\"psi\"], errors=\"coerce\")\n",
    "    drift[\"tv_distance\"] = pd.to_numeric(drift[\"tv_distance\"], errors=\"coerce\")\n",
    "\n",
    "    drift[\"severity\"] = \"\"\n",
    "    severity = np.full(len(drift), \"\", dtype=object)\n",
    "\n",
    "    psi_v = drift[\"psi\"].to_numpy(dtype=float)\n",
    "    tv_v = drift[\"tv_distance\"].to_numpy(dtype=float)\n",
    "    is_num = drift[\"type\"].to_numpy(dtype=str) == \"numeric\"\n",
    "    is_cat = ~is_num\n",
    "\n",
    "    m = is_num & np.isfinite(psi_v)\n",
    "    severity[m & (psi_v >= 0.2)] = \"high\"\n",
    "    severity[m & (psi_v >= 0.1) & (psi_v < 0.2)] = \"medium\"\n",
    "\n",
    "    m = is_cat & np.isfinite(tv_v)\n",
    "    severity[m & (tv_v >= 0.15)] = \"high\"\n",
    "    severity[m & (tv_v >= 0.08) & (tv_v < 0.15)] = \"medium\"\n",
    "\n",
    "    drift[\"severity\"] = severity\n",
    "    sev_rank = {\"high\": 2, \"medium\": 1, \"\": 0}\n",
    "    drift[\"sev_rank\"] = drift[\"severity\"].map(sev_rank).fillna(0).astype(int)\n",
    "    drift = drift.sort_values([\"sev_rank\", \"psi\", \"tv_distance\"], ascending=[False, False, False]).drop(columns=[\"sev_rank\"])\n",
    "    drift.to_csv(drift_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    hi = drift[drift[\"severity\"].isin([\"high\", \"medium\"])].head(20)\n",
    "    show_table(hi, title=\"Drift highlights (medium/high)\", max_rows=50)\n",
    "    print(\"Saved:\", drift_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376abd5",
   "metadata": {},
   "source": [
    "<h2 style='margin:22px 0 10px 0;'>6) Triage threshold (calibrated risk × capacity)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d43e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "triage_json = OUT_DIR / \"triage_threshold_policy.json\"\n",
    "triage_preview = OUT_DIR / \"triage_actions_preview.csv\"\n",
    "\n",
    "if len(train_hist) == 0 or len(test_future) == 0 or \"is_failure\" not in df_time.columns:\n",
    "    triage_json.write_text(\n",
    "        json.dumps({\"status\": \"skipped\", \"reason\": \"missing_windows_or_target\"}, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    pd.DataFrame(columns=[\"interaction_id\", \"proba_failure\", \"action_review\"]).to_csv(triage_preview, index=False)\n",
    "    print(\"Triage skipped.\")\n",
    "else:\n",
    "    train_df = train_hist.sort_values(\"timestamp_utc\").copy()\n",
    "    test_df = test_future.sort_values(\"timestamp_utc\").copy()\n",
    "\n",
    "    y_train = pd.to_numeric(train_df[\"is_failure\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    y_test = pd.to_numeric(test_df[\"is_failure\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    NUM = [\n",
    "        c\n",
    "        for c in [\n",
    "            \"latency_ms\",\n",
    "            \"prompt_tokens\",\n",
    "            \"completion_tokens\",\n",
    "            \"total_tokens\",\n",
    "            \"cost_usd\",\n",
    "            \"tool_calls_count\",\n",
    "            \"tokens_per_second\",\n",
    "            \"prompt_to_completion_ratio\",\n",
    "        ]\n",
    "        if c in train_df.columns\n",
    "    ]\n",
    "    CAT = [\n",
    "        c\n",
    "        for c in [\n",
    "            \"model_provider\",\n",
    "            \"model_name\",\n",
    "            \"use_case\",\n",
    "            \"channel\",\n",
    "            \"account_tier\",\n",
    "            \"region\",\n",
    "            \"segment\",\n",
    "        ]\n",
    "        if c in train_df.columns\n",
    "    ]\n",
    "\n",
    "    if len(NUM) == 0 and len(CAT) == 0:\n",
    "        triage_json.write_text(\n",
    "            json.dumps({\"status\": \"skipped\", \"reason\": \"missing_feature_columns\"}, indent=2),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        pd.DataFrame(columns=[\"interaction_id\", \"proba_failure\", \"action_review\"]).to_csv(triage_preview, index=False)\n",
    "        print(\"Triage skipped: missing features.\")\n",
    "    elif y_train.nunique() < 2 or y_test.nunique() < 2:\n",
    "        triage_json.write_text(\n",
    "            json.dumps({\"status\": \"skipped\", \"reason\": \"insufficient_class_variety\"}, indent=2),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        pd.DataFrame(columns=[\"interaction_id\", \"proba_failure\", \"action_review\"]).to_csv(triage_preview, index=False)\n",
    "        print(\"Triage skipped: insufficient class variety.\")\n",
    "    else:\n",
    "        X_train = train_df[NUM + CAT].copy()\n",
    "        X_test = test_df[NUM + CAT].copy()\n",
    "\n",
    "        for c in NUM:\n",
    "            X_train[c] = pd.to_numeric(X_train[c], errors=\"coerce\")\n",
    "            X_test[c] = pd.to_numeric(X_test[c], errors=\"coerce\")\n",
    "\n",
    "        train_ts = pd.to_datetime(train_df[\"timestamp_utc\"], utc=True, errors=\"coerce\")\n",
    "        cal_cut_ts = train_ts.quantile(0.85)\n",
    "\n",
    "        cal_mask = (train_ts > cal_cut_ts).to_numpy()\n",
    "        test_fold = np.where(cal_mask, 0, -1)\n",
    "        ps = PredefinedSplit(test_fold)\n",
    "\n",
    "        transformers = []\n",
    "        if len(NUM):\n",
    "            num_pipe = Pipeline(\n",
    "                [\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                ]\n",
    "            )\n",
    "            transformers.append((\"num\", num_pipe, NUM))\n",
    "\n",
    "        if len(CAT):\n",
    "            cat_pipe = Pipeline(\n",
    "                [\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "                ]\n",
    "            )\n",
    "            transformers.append((\"cat\", cat_pipe, CAT))\n",
    "\n",
    "        pre = ColumnTransformer(transformers)\n",
    "\n",
    "        base = Pipeline(\n",
    "            [\n",
    "                (\"pre\", pre),\n",
    "                (\n",
    "                    \"clf\",\n",
    "                    LogisticRegression(\n",
    "                        solver=\"saga\",\n",
    "                        max_iter=5000,\n",
    "                        n_jobs=-1,\n",
    "                        tol=1e-3,\n",
    "                        class_weight=\"balanced\",\n",
    "                        random_state=RANDOM_SEED,\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        cal = CalibratedClassifierCV(base, method=\"sigmoid\", cv=ps)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "            cal.fit(X_train, y_train)\n",
    "\n",
    "        proba = cal.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        metrics = {\n",
    "            \"roc_auc\": float(roc_auc_score(y_test, proba)),\n",
    "            \"avg_precision\": float(average_precision_score(y_test, proba)),\n",
    "            \"test_base_rate\": float(y_test.mean()),\n",
    "            \"test_size\": int(len(y_test)),\n",
    "        }\n",
    "        metrics_df = pd.DataFrame({\"metric\": list(metrics.keys()), \"value\": list(metrics.values())})\n",
    "        show_table(metrics_df, title=\"Failure-triage model metrics (evaluation window)\", max_rows=20)\n",
    "\n",
    "        days = pd.to_datetime(test_df[\"timestamp_utc\"], utc=True, errors=\"coerce\").dt.floor(\"D\").values\n",
    "        ths = np.linspace(0.05, 0.95, 91)\n",
    "\n",
    "        rows = []\n",
    "        for th in ths:\n",
    "            pred = (proba >= th).astype(int)\n",
    "            tn, fp, fn, tp = confusion_matrix(y_test, pred, labels=[0, 1]).ravel()\n",
    "\n",
    "            expected_cost = (tp + fp) * REVIEW_COST + fn * TRIAGE_FN_COST_USD\n",
    "\n",
    "            tmp = pd.DataFrame({\"day\": days, \"review\": pred})\n",
    "            day_sum = tmp.groupby(\"day\")[\"review\"].sum()\n",
    "            mean_reviews = float(day_sum.mean())\n",
    "            p90_reviews = float(np.percentile(day_sum.values, 90)) if len(day_sum) else float(\"nan\")\n",
    "\n",
    "            feasible = (mean_reviews <= CAPACITY_PER_DAY) and (p90_reviews <= CAPACITY_P90_GUARD)\n",
    "            rows.append((th, expected_cost, mean_reviews, p90_reviews, feasible, tn, fp, fn, tp))\n",
    "\n",
    "        cost_df = pd.DataFrame(\n",
    "            rows,\n",
    "            columns=[\n",
    "                \"threshold\",\n",
    "                \"expected_cost\",\n",
    "                \"mean_reviews_per_day\",\n",
    "                \"p90_reviews_per_day\",\n",
    "                \"feasible\",\n",
    "                \"tn\",\n",
    "                \"fp\",\n",
    "                \"fn\",\n",
    "                \"tp\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        feasible_df = cost_df[cost_df[\"feasible\"]]\n",
    "        best = feasible_df.sort_values(\"expected_cost\").head(1) if len(feasible_df) else cost_df.sort_values(\"expected_cost\").head(1)\n",
    "        best_th = float(best[\"threshold\"].iloc[0])\n",
    "\n",
    "        line_chart(cost_df, x=\"threshold\", ys=[\"expected_cost\"], title=\"Expected cost vs threshold\")\n",
    "        line_chart(\n",
    "            cost_df,\n",
    "            x=\"threshold\",\n",
    "            ys=[\"mean_reviews_per_day\"],\n",
    "            title=\"Review load (mean) vs threshold\",\n",
    "            hlines=[(CAPACITY_PER_DAY, \"--\")],\n",
    "        )\n",
    "        line_chart(\n",
    "            cost_df,\n",
    "            x=\"threshold\",\n",
    "            ys=[\"p90_reviews_per_day\"],\n",
    "            title=\"Review load (p90) vs threshold\",\n",
    "            hlines=[(CAPACITY_P90_GUARD, \"--\")],\n",
    "        )\n",
    "\n",
    "        triage_json.write_text(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"status\": \"ok\",\n",
    "                    \"label_definition\": \"is_failure (post-call failure triage)\",\n",
    "                    \"threshold\": best_th,\n",
    "                    \"time_split\": {\n",
    "                        \"history_end_timestamp_utc\": str(cut_ts) if cut_ts is not None else None,\n",
    "                        \"calibration_cut_timestamp_utc\": str(cal_cut_ts),\n",
    "                    },\n",
    "                    \"features_num\": NUM,\n",
    "                    \"features_cat\": CAT,\n",
    "                    \"economics\": {\n",
    "                        \"review_cost\": REVIEW_COST,\n",
    "                        \"fn_cost\": TRIAGE_FN_COST_USD,\n",
    "                        \"capacity_per_day\": CAPACITY_PER_DAY,\n",
    "                        \"capacity_p90_guard\": CAPACITY_P90_GUARD,\n",
    "                    },\n",
    "                    \"metrics\": metrics,\n",
    "                },\n",
    "                indent=2,\n",
    "            ),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "\n",
    "        preview_cols = [\"interaction_id\", \"timestamp_utc\", \"use_case\", \"model_provider\", \"model_name\", \"cost_usd\", \"latency_ms\"]\n",
    "        for c in preview_cols:\n",
    "            if c not in test_df.columns:\n",
    "                test_df[c] = np.nan if c in [\"cost_usd\", \"latency_ms\"] else \"unknown\"\n",
    "\n",
    "        preview = test_df[preview_cols].copy()\n",
    "        preview[\"proba_failure\"] = proba\n",
    "        preview[\"action_review\"] = preview[\"proba_failure\"] >= best_th\n",
    "        preview.sort_values([\"action_review\", \"proba_failure\"], ascending=[False, False]).head(2000).to_csv(\n",
    "            triage_preview, index=False, encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "        pred_best = (proba >= best_th).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, pred_best, labels=[0, 1]).ravel()\n",
    "        thr_kpis = pd.DataFrame(\n",
    "            [\n",
    "                (\"threshold\", best_th),\n",
    "                (\"tn\", int(tn)),\n",
    "                (\"fp\", int(fp)),\n",
    "                (\"fn\", int(fn)),\n",
    "                (\"tp\", int(tp)),\n",
    "                (\"precision\", float(tp / max(tp + fp, 1))),\n",
    "                (\"recall\", float(tp / max(tp + fn, 1))),\n",
    "            ],\n",
    "            columns=[\"metric\", \"value\"],\n",
    "        )\n",
    "        show_table(thr_kpis, title=\"Chosen threshold performance (evaluation window)\", max_rows=50)\n",
    "\n",
    "        print(\"Saved:\", triage_json, triage_preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e6a405",
   "metadata": {},
   "source": [
    "<h2 style='margin:22px 0 10px 0;'>DecisionArtifact JSON</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35da6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_path = OUT_DIR / \"decision_artifact.json\"\n",
    "\n",
    "policy_df = _read_csv_safe(OUT_DIR / \"routing_policy_use_case.csv\")\n",
    "drift_df = _read_csv_safe(OUT_DIR / \"drift_report.csv\")\n",
    "triage_obj = _read_json_safe(OUT_DIR / \"triage_threshold_policy.json\")\n",
    "\n",
    "issues = []\n",
    "if policy_df.empty:\n",
    "    issues.append(\"routing_policy_empty_or_missing\")\n",
    "if drift_df.empty:\n",
    "    issues.append(\"drift_report_empty_or_missing\")\n",
    "\n",
    "triage_status = triage_obj.get(\"status\", \"missing\")\n",
    "if triage_status != \"ok\":\n",
    "    issues.append(f\"triage_status_{triage_status}\")\n",
    "\n",
    "artifact_status = \"ok\" if len(issues) == 0 else \"partial\"\n",
    "\n",
    "artifact = {\n",
    "    \"status\": artifact_status,\n",
    "    \"issues\": issues,\n",
    "    \"dataset\": \"LLM Production Telemetry (Synthetic)\",\n",
    "    \"notebook\": \"Decision-Grade Observability (Notebook Edition)\",\n",
    "    \"windows\": {\n",
    "        \"history_end_timestamp_utc\": str(cut_ts) if cut_ts is not None else None,\n",
    "        \"history_start_timestamp_utc\": str(train_hist[\"timestamp_utc\"].min()) if len(train_hist) else None,\n",
    "        \"evaluation_start_timestamp_utc\": str(test_future[\"timestamp_utc\"].min()) if len(test_future) else None,\n",
    "        \"evaluation_end_timestamp_utc\": str(test_future[\"timestamp_utc\"].max()) if len(test_future) else None,\n",
    "    },\n",
    "    \"generated_outputs\": {\n",
    "        \"routing_policy_use_case_csv\": str(OUT_DIR / \"routing_policy_use_case.csv\"),\n",
    "        \"drift_report_csv\": str(OUT_DIR / \"drift_report.csv\"),\n",
    "        \"triage_threshold_policy_json\": str(OUT_DIR / \"triage_threshold_policy.json\"),\n",
    "        \"triage_actions_preview_csv\": str(OUT_DIR / \"triage_actions_preview.csv\"),\n",
    "    },\n",
    "    \"operator_knobs\": {\n",
    "        \"SLA_MS\": float(SLA_MS),\n",
    "        \"FAIL_BUDGET\": float(FAIL_BUDGET),\n",
    "        \"SLA_BUDGET\": float(SLA_BUDGET),\n",
    "        \"TIME_SPLIT_FRAC\": float(TIME_SPLIT_FRAC),\n",
    "        \"ROUTING_MAX_FAIL_RATE\": float(ROUTING_MAX_FAIL_RATE),\n",
    "        \"HOTSPOT_FAIL_RATE\": float(HOTSPOT_FAIL_RATE),\n",
    "        \"MAX_LATENCY_MISSING_RATE\": float(MAX_LATENCY_MISSING_RATE),\n",
    "        \"MAX_COST_MISSING_RATE\": float(MAX_COST_MISSING_RATE),\n",
    "        \"FAILURE_EVENT_COST_USD\": float(FAILURE_EVENT_COST_USD),\n",
    "        \"SLA_BREACH_PENALTY_USD\": float(SLA_BREACH_PENALTY_USD),\n",
    "        \"MIN_REQUESTS_POLICY\": int(MIN_REQUESTS_POLICY),\n",
    "        \"MIN_REQUESTS_SLICE\": int(MIN_REQUESTS_SLICE),\n",
    "        \"REVIEW_COST\": float(REVIEW_COST),\n",
    "        \"TRIAGE_FN_COST_USD\": float(TRIAGE_FN_COST_USD),\n",
    "        \"CAPACITY_PER_DAY\": int(CAPACITY_PER_DAY),\n",
    "        \"CAPACITY_P90_GUARD\": int(CAPACITY_P90_GUARD),\n",
    "        \"DAILY_COST_BUDGET_USD\": float(DAILY_COST_BUDGET_USD),\n",
    "    },\n",
    "    \"summaries\": {\n",
    "        \"routing_policy_rows\": int(len(policy_df)),\n",
    "        \"routing_policy_use_cases\": int(policy_df[\"use_case\"].nunique()) if \"use_case\" in policy_df.columns else 0,\n",
    "        \"drift_rows\": int(len(drift_df)),\n",
    "        \"triage_status\": triage_status,\n",
    "        \"triage_threshold\": triage_obj.get(\"threshold\", None),\n",
    "        \"triage_metrics\": triage_obj.get(\"metrics\", {}),\n",
    "        \"triage_label_definition\": triage_obj.get(\"label_definition\", None),\n",
    "    },\n",
    "}\n",
    "\n",
    "decision_path.write_text(json.dumps(artifact, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved:\", decision_path)\n",
    "print(\"Artifacts:\", sorted([p.name for p in OUT_DIR.glob(\"*\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971f809",
   "metadata": {},
   "source": [
    "<h2 style=\"margin:24px 0 10px 0;\">End — ship the policy, not the charts</h2>\n",
    "\n",
    "<div style=\"padding:14px 16px; border:1px solid rgba(255,255,255,0.12); border-radius:14px; background:rgba(255,255,255,0.04);\">\n",
    "  You started with noisy logs. You end with <b>two enforceable policies</b> you can run in production:\n",
    "  a routing policy and a triage policy. Everything above was evidence that these policies respect reliability, cost, and capacity.\n",
    "  <br/><br/>\n",
    "\n",
    "  <b>What to ship (the only deliverables that matter)</b><br/>\n",
    "  • <code>routing_policy_use_case.csv</code> → routing config per <code>use_case</code> (cost-aware + SLO-aware).<br/>\n",
    "  • <code>triage_threshold_policy.json</code> → review threshold (capacity-aware, cost-weighted).<br/>\n",
    "  • <code>decision_artifact.json</code> → audit record (window, knobs, outcomes) for automation and traceability.\n",
    "  <br/><br/>\n",
    "\n",
    "  <b>How to operationalize safely</b><br/>\n",
    "  1) Deploy behind a feature flag + rollback switch.<br/>\n",
    "  2) Enforce capacity guards (mean and p90 queue load) before rollout.<br/>\n",
    "  3) Monitor burn + drift + queue load in the next window, then keep/rollback.\n",
    "  <br/><br/>\n",
    "\n",
    "  <b>Stop-ship rules</b><br/>\n",
    "  • Integrity gates fail.<br/>\n",
    "  • Error-budget burn &gt; 1 for failures or SLA breaches.<br/>\n",
    "  • Drift severity is <b>high</b> on core features (latency/tokens/cost/model mix).<br/>\n",
    "  • Predicted queue p90 exceeds <code>CAPACITY_P90_GUARD</code>.\n",
    "  <br/><br/>\n",
    "\n",
    "  <b>When to re-run</b><br/>\n",
    "  Traffic mix changes • provider/model/pricing changes • failure base-rate shifts • review capacity changes.\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
